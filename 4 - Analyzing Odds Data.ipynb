{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing Odds Data\n",
    "\n",
    "In the last notebook, we obtained historical odds data from oddsshark, and then augmented our game level data to include the implied probabilities, and over/under lines.\n",
    "We saved that data to a file called 'df_bp3.csv'\n",
    "In this notebook, we will do some initial exploration of that odds data, and compare the quality of our first model predictions to the implied probabilities given by the oddsmakers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import lightgbm as lgbm\n",
    "import structureboost as stb\n",
    "import ml_insights as mli\n",
    "from structureboost import log_loss\n",
    "pd.set_option('display.max_columns',1000)\n",
    "pd.set_option('display.max_rows',1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('df_bp3.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks to see if '0-probability' occurs in correct spots\n",
    "\n",
    "pd.crosstab(df.implied_prob_h>0, df.season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_odds = df[df.season>=2019]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_odds.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_odds.implied_prob_h_mid, bins=np.linspace(.15,.85,85));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_odds.implied_prob_h + df_odds.implied_prob_v, bins=np.linspace(1,1.06,61));\n",
    "\n",
    "# interesting to see second peak around 1.04-1.05 ... IDEAS??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.run_diff!=0]\n",
    "df_train = df[(df.season>1901) & (df.season<=2000)]\n",
    "df_valid = df[(df.season>=2001) & (df.season<=2020)]\n",
    "df_test = df[df.season>=2021]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['OBP_162_h','OBP_162_v',\n",
    "            'SLG_162_h','SLG_162_v', \n",
    "            # 'OBP_30_h','OBP_30_v',\n",
    "            # 'SLG_30_h','SLG_30_v',\n",
    "            # 'game_no_h',\n",
    "           ]\n",
    "target = 'home_victory'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.loc[:,features]\n",
    "X_valid = df_valid.loc[:,features]\n",
    "X_test = df_test.loc[:,features]\n",
    "\n",
    "y_train = df_train[target].to_numpy()\n",
    "y_valid = df_valid[target].to_numpy()\n",
    "y_test = df_test[target].to_numpy()\n",
    "X_train.shape, X_valid.shape, X_test.shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm1 = lgbm.LGBMClassifier(n_estimators=1000, learning_rate=.02, max_depth=3)\n",
    "lgbm1.fit(X_train, y_train, eval_set=(X_valid, y_valid), eval_metric='logloss', \n",
    "          callbacks=[lgbm.early_stopping(stopping_rounds=50), lgbm.log_evaluation(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_lgbm = lgbm1.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv_mean = y_train.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_loss = log_loss(y_test, preds_lgbm)\n",
    "naive_loss = log_loss(y_test, hv_mean*np.ones(len(y_test)))\n",
    "lgbm_loss, naive_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lv_loss = log_loss(y_test, df_test.implied_prob_h_mid)\n",
    "lv_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(naive_loss - lgbm_loss)/(naive_loss - lv_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,6))\n",
    "mli.plot_reliability_diagram(y_test, df_test.implied_prob_h_mid, show_histogram=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_disc = np.abs(preds_lgbm - df_test.implied_prob_h_mid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(test_disc, np.linspace(0,.35,36));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[test_disc>0.25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Fancy' code that basically says, if the home team is favored then take select their SP. If not, select the oppo SP.\n",
    "\n",
    "# These are the pitchers that were favored in these games\n",
    "[row[1]['pitcher_start_name_h']  if row[1]['implied_prob_h_mid']>.5 \n",
    " else row[1]['pitcher_start_name_v'] \n",
    " for row in df_test[test_disc>.25].iterrows() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the underdog pitchers in these games\n",
    "[row[1]['pitcher_start_name_h']  if row[1]['implied_prob_h_mid']<.5 \n",
    " else row[1]['pitcher_start_name_v'] for row in df_test[test_disc>.25].iterrows() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df_test.home_victory[test_disc>.2], \n",
    "            np.round(df_test.implied_prob_h_mid[test_disc>.2], decimals=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df_test.home_victory[test_disc>.2], \n",
    "            np.round(preds_lgbm[test_disc>.2], decimals=1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis \n",
    "\n",
    "- LV odds are \"better\" than our current model\n",
    "- Largest discrepancies appear when we have a strong pitcher vs a weak pitcher\n",
    "- LV probs seem to be \"right\" in those cases\n",
    "\n",
    "CONCLUSION: Need to factor in the starting pitcher to improve our model!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
